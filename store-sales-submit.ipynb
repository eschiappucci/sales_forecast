{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943b2c20",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-26T08:00:40.913504Z",
     "iopub.status.busy": "2024-07-26T08:00:40.913159Z",
     "iopub.status.idle": "2024-07-26T08:01:00.255565Z",
     "shell.execute_reply": "2024-07-26T08:01:00.254755Z"
    },
    "papermill": {
     "duration": 19.349394,
     "end_time": "2024-07-26T08:01:00.257802",
     "exception": false,
     "start_time": "2024-07-26T08:00:40.908408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:14:13.921617: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-05 17:14:13.929426: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-05 17:14:13.996009: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-05 17:14:14.049797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-05 17:14:14.103961: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-05 17:14:14.118855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-05 17:14:14.212789: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 17:14:15.087212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_path = 'og_data/'\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates=['date'])\n",
    "test = pd.read_csv(os.path.join(data_path, 'test.csv'), parse_dates=['date'])\n",
    "\n",
    "# holidays_events = pd.read_csv(os.path.join(data_path, 'holidays_events.csv'), parse_dates=['date'])\n",
    "oil = pd.read_csv(os.path.join(data_path, 'oil.csv'), parse_dates=['date'])\n",
    "stores = pd.read_csv(os.path.join(data_path, 'stores.csv'))\n",
    "transactions = pd.read_csv(os.path.join(data_path, 'transactions.csv'), parse_dates=['date'])\n",
    "submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'), index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2756fdd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:01:00.266033Z",
     "iopub.status.busy": "2024-07-26T08:01:00.265752Z",
     "iopub.status.idle": "2024-07-26T08:01:03.324404Z",
     "shell.execute_reply": "2024-07-26T08:01:03.323452Z"
    },
    "papermill": {
     "duration": 3.065145,
     "end_time": "2024-07-26T08:01:03.326557",
     "exception": false,
     "start_time": "2024-07-26T08:01:00.261412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 1506.5585637191425\n",
      "P-value: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates=['date'])\n",
    "\n",
    "data['dayofweek'] = data['date'].dt.dayofweek\n",
    "\n",
    "average_sales = data.groupby('dayofweek')['sales'].mean()\n",
    "\n",
    "average_sales.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Sales by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xticks(range(0, 7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=0)\n",
    "plt.savefig('../weekday.png')\n",
    "\n",
    "grouped = data.groupby('dayofweek')['sales']\n",
    "\n",
    "sales_data = [group for name, group in grouped]\n",
    "\n",
    "f_stat, p_value = f_oneway(*sales_data)\n",
    "print('F-statistic:', f_stat)\n",
    "print('P-value:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d88fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a83075",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min, date_max = train.date.min(), test.date.max()\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "new_df = pd.DataFrame()\n",
    "new_df['date'] = pd.date_range(date_min, date_max)\n",
    "new_df = pd.merge(new_df,oil, how=\"outer\", on='date')\n",
    "new_df['dcoilwtico'] = new_df['dcoilwtico'].interpolate(method = \"linear\")\n",
    "oil = new_df\n",
    "\n",
    "oil_prices = np.asarray(oil['dcoilwtico'])\n",
    "nans, x= nan_helper(oil_prices)\n",
    "oil_prices[nans]= np.interp(x(nans), x(~nans), oil_prices[~nans])\n",
    "oil['dcoilwtico'] = oil_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e1ed22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:01:03.350481Z",
     "iopub.status.busy": "2024-07-26T08:01:03.350216Z",
     "iopub.status.idle": "2024-07-26T08:01:05.696014Z",
     "shell.execute_reply": "2024-07-26T08:01:05.695126Z"
    },
    "papermill": {
     "duration": 2.35348,
     "end_time": "2024-07-26T08:01:05.698750",
     "exception": false,
     "start_time": "2024-07-26T08:01:03.345270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of the original train data:\n",
      "(1599, 1782)\n",
      "store_nbr                          1                                       \\\n",
      "family                            0    1    2       3    4        5    6    \n",
      "date       dcoilwtico dayofweek                                             \n",
      "2013-01-01 93.140000  1          0.0  0.0  0.0     0.0  0.0    0.000  0.0   \n",
      "2013-01-02 93.140000  2          2.0  0.0  2.0  1091.0  0.0  470.652  0.0   \n",
      "2013-01-03 92.970000  3          3.0  0.0  0.0   919.0  0.0  310.655  0.0   \n",
      "2013-01-04 93.120000  4          3.0  0.0  3.0   953.0  0.0  198.366  0.0   \n",
      "2013-01-05 93.146667  5          5.0  0.0  3.0  1160.0  0.0  301.057  0.0   \n",
      "\n",
      "store_nbr                                                ...    9           \\\n",
      "family                               7      8        9   ...   23       24   \n",
      "date       dcoilwtico dayofweek                          ...                 \n",
      "2013-01-01 93.140000  1             0.0    0.0    0.000  ...  0.0    0.000   \n",
      "2013-01-02 93.140000  2          1060.0  579.0  164.069  ...  0.0  374.531   \n",
      "2013-01-03 92.970000  3           836.0  453.0  151.582  ...  0.0  400.863   \n",
      "2013-01-04 93.120000  4           827.0  460.0  131.411  ...  0.0  310.878   \n",
      "2013-01-05 93.146667  5           811.0  464.0  118.613  ...  0.0  447.685   \n",
      "\n",
      "store_nbr                                                                  \\\n",
      "family                              25   26   27       28    29   30   31   \n",
      "date       dcoilwtico dayofweek                                             \n",
      "2013-01-01 93.140000  1            0.0  0.0  0.0    0.000   0.0  0.0  0.0   \n",
      "2013-01-02 93.140000  2          482.0  0.0  0.0  651.292  83.0  0.0  0.0   \n",
      "2013-01-03 92.970000  3          372.0  0.0  0.0  509.496  66.0  0.0  0.0   \n",
      "2013-01-04 93.120000  4          324.0  0.0  0.0  332.672  57.0  0.0  0.0   \n",
      "2013-01-05 93.146667  5          461.0  0.0  0.0  510.919  84.0  0.0  0.0   \n",
      "\n",
      "store_nbr                                \n",
      "family                               32  \n",
      "date       dcoilwtico dayofweek          \n",
      "2013-01-01 93.140000  1           0.000  \n",
      "2013-01-02 93.140000  2          29.214  \n",
      "2013-01-03 92.970000  3          25.000  \n",
      "2013-01-04 93.120000  4          11.000  \n",
      "2013-01-05 93.146667  5          28.361  \n",
      "\n",
      "[5 rows x 1782 columns]\n",
      "\n",
      "First 5 rows of the original train data:\n",
      "(1599, 1782)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.11111111 0.         0.16666667 ... 0.         0.         0.48512928]\n",
      " [0.16666667 0.         0.         ... 0.         0.         0.41515136]\n",
      " ...\n",
      " [0.22222222 0.         0.25       ... 0.49480247 0.00920245 0.45467379]\n",
      " [0.05555556 0.         0.08333333 ... 0.59724127 0.02147239 0.65942643]\n",
      " [0.11111111 0.         0.41666667 ... 0.34804615 0.00613497 0.31712582]]\n"
     ]
    }
   ],
   "source": [
    "train_data = train.copy().drop(['onpromotion'], axis=1)\n",
    "test_data = test.copy().drop(['onpromotion'], axis=1)\n",
    "\n",
    "train_data['dayofweek'] = data['date'].dt.dayofweek\n",
    "test_data['dayofweek'] = data['date'].dt.dayofweek\n",
    "\n",
    "train_data = train_data.merge(oil, on='date', how='left')\n",
    "test_data = test_data.merge(oil, on='date', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Ordinal Encoding for 'family' column\n",
    "ordinal_encoder = OrdinalEncoder(dtype=int)\n",
    "train_data[['family']] = ordinal_encoder.fit_transform(train_data[['family']])\n",
    "test_data[['family']] = ordinal_encoder.transform(test_data[['family']])\n",
    "\n",
    "num_days_train = train[\"date\"].nunique()\n",
    "num_stores_train = train[\"store_nbr\"].nunique()\n",
    "num_families_train = train[\"family\"].nunique()\n",
    "\n",
    "num_days_test = test[\"date\"].nunique()\n",
    "num_stores_test = test[\"store_nbr\"].nunique()\n",
    "num_families_test = test[\"family\"].nunique()\n",
    "\n",
    "pivoted_train = train_data.pivot(index=['date','dcoilwtico','dayofweek'], columns=['store_nbr', 'family'], values='sales')\n",
    "\n",
    "train_samples = int(num_days_train * 0.95)\n",
    "train_samples_df = pivoted_train[:train_samples]\n",
    "valid_samples_df = pivoted_train[train_samples:]\n",
    "\n",
    "print(\"\\nFirst 5 rows of the original train data:\")\n",
    "print(train_samples_df.shape)\n",
    "print(train_samples_df.head())\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler.fit(train_samples_df)\n",
    "scaled_train_samples = minmax_scaler.transform(train_samples_df)\n",
    "scaled_validation_samples = minmax_scaler.transform(valid_samples_df)\n",
    "\n",
    "print(\"\\nFirst 5 rows of the original train data:\")\n",
    "print(scaled_train_samples.shape)\n",
    "print(scaled_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccd28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, count, max_var = 0, 0, 20\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 1e-2, 1e-5 * batch_size, 1e-4\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 0, 0, 0.75\n",
    "    line_max, line_min = 1, 1e-2\n",
    "    \n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'line': \n",
    "            m = (lr_min - lr_start)/epochs\n",
    "            lr = m * epoch + lr_start\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = (epochs - lr_ramp_ep - lr_sus_ep) /(epochs/20), epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            m = (line_min - line_max)/epochs\n",
    "            control = m * epoch + line_max\n",
    "            lr = ((lr_start - lr_min) * (1+np.cos(phase)) + lr_min) * control\n",
    "        elif mode == 'var':\n",
    "            global a, count, max_var\n",
    "            m = (lr_min - lr_start)/max_var\n",
    "            if a < max_var:\n",
    "                lr = m * (epoch-count) + lr_start\n",
    "                a += 1\n",
    "            elif a >= max_var:\n",
    "                print(epoch-count)\n",
    "                lr = m * (epoch-count) + lr_start\n",
    "                count += max_var + 1\n",
    "                a = 0\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "149d8112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "lr_mode = 'line'\n",
    "epochs = 1000 \n",
    "lr_cb = get_lr_callback(batch_size, mode=lr_mode, epochs=epochs, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad32ef96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:01:05.709144Z",
     "iopub.status.busy": "2024-07-26T08:01:05.708458Z",
     "iopub.status.idle": "2024-07-26T08:05:05.541386Z",
     "shell.execute_reply": "2024-07-26T08:05:05.540347Z"
    },
    "papermill": {
     "duration": 239.840818,
     "end_time": "2024-07-26T08:05:05.543883",
     "exception": false,
     "start_time": "2024-07-26T08:01:05.703065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ separable_conv1d_65             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,009,124</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_66             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">290,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_67             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">290,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_68             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">552,960</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_69             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,104,896</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_70             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,104,896</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_71             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">580,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_72             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">290,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_73             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">290,304</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_14                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_15                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cropping1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_12             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1782</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">914,166</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ separable_conv1d_65             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,009,124\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_66             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m290,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_67             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m290,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_21 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_68             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │       \u001b[38;5;34m552,960\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_69             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m1,104,896\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_70             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m1,104,896\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_22 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_71             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m580,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_72             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m290,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_73             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m290,304\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_23 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_14                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_15                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_15 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cropping1d_2 (\u001b[38;5;33mCropping1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_12             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1782\u001b[0m)       │       \u001b[38;5;34m914,166\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,581,274</span> (36.55 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,581,274\u001b[0m (36.55 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,579,226</span> (36.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,579,226\u001b[0m (36.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 12s/step - loss: 0.3694 - msle: 0.0764 - val_loss: 0.2486 - val_msle: 0.0424 - learning_rate: 0.0100\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 13s/step - loss: 1.3090 - msle: 0.2840 - val_loss: 0.8313 - val_msle: 0.1333 - learning_rate: 0.0100\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 14s/step - loss: 1.8676 - msle: 0.4225 - val_loss: 0.8864 - val_msle: 0.1118 - learning_rate: 0.0100\n",
      "Epoch 4/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:21:49.891282: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 13s/step - loss: 1.6697 - msle: 0.3167 - val_loss: 0.7203 - val_msle: 0.0978 - learning_rate: 0.0100\n",
      "Epoch 5/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.7670 - msle: 0.1715 - val_loss: 1.3330 - val_msle: 0.2595 - learning_rate: 0.0100\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11s/step - loss: 1.1720 - msle: 0.2717 - val_loss: 1.0394 - val_msle: 0.1961 - learning_rate: 0.0100\n",
      "Epoch 7/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 1.0132 - msle: 0.2095 - val_loss: 0.4718 - val_msle: 0.0697 - learning_rate: 0.0099\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 0.8285 - msle: 0.2141 - val_loss: 0.5149 - val_msle: 0.0828 - learning_rate: 0.0099\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.7764 - msle: 0.1964 - val_loss: 0.5244 - val_msle: 0.0792 - learning_rate: 0.0099\n",
      "Epoch 10/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.4995 - msle: 0.1095 - val_loss: 0.5860 - val_msle: 0.0830 - learning_rate: 0.0099\n",
      "Epoch 11/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:25:56.479018: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 1.0306 - msle: 0.2864 - val_loss: 0.9817 - val_msle: 0.1207 - learning_rate: 0.0099\n",
      "Epoch 12/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:26:32.009558: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 1.0906 - msle: 0.2441 - val_loss: 0.6365 - val_msle: 0.1068 - learning_rate: 0.0099\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.8050 - msle: 0.1696 - val_loss: 0.6329 - val_msle: 0.0957 - learning_rate: 0.0099\n",
      "Epoch 14/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:27:43.280659: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.9203 - msle: 0.1623 - val_loss: 0.5205 - val_msle: 0.1103 - learning_rate: 0.0099\n",
      "Epoch 15/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:28:19.296003: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 1.2827 - msle: 0.3352 - val_loss: 0.5080 - val_msle: 0.0885 - learning_rate: 0.0099\n",
      "Epoch 16/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:28:55.502667: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 1.1696 - msle: 0.2781 - val_loss: 0.7397 - val_msle: 0.1614 - learning_rate: 0.0099\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.5983 - msle: 0.1369 - val_loss: 0.4819 - val_msle: 0.0939 - learning_rate: 0.0098\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.4460 - msle: 0.0942 - val_loss: 0.4255 - val_msle: 0.0733 - learning_rate: 0.0098\n",
      "Epoch 19/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:30:44.291422: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3742 - msle: 0.0718 - val_loss: 0.3852 - val_msle: 0.0660 - learning_rate: 0.0098\n",
      "Epoch 20/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:31:20.780429: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3682 - msle: 0.0723 - val_loss: 0.3753 - val_msle: 0.0634 - learning_rate: 0.0098\n",
      "Epoch 21/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:31:56.975287: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3566 - msle: 0.0708 - val_loss: 0.3407 - val_msle: 0.0580 - learning_rate: 0.0098\n",
      "Epoch 22/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:32:33.244649: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3103 - msle: 0.0575 - val_loss: 0.3213 - val_msle: 0.0545 - learning_rate: 0.0098\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2826 - msle: 0.0512 - val_loss: 0.3179 - val_msle: 0.0541 - learning_rate: 0.0098\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2644 - msle: 0.0471 - val_loss: 0.3153 - val_msle: 0.0541 - learning_rate: 0.0098\n",
      "Epoch 25/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:34:21.656052: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2444 - msle: 0.0420 - val_loss: 0.3102 - val_msle: 0.0530 - learning_rate: 0.0098\n",
      "Epoch 26/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:34:57.540217: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2251 - msle: 0.0370 - val_loss: 0.3030 - val_msle: 0.0517 - learning_rate: 0.0098\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2152 - msle: 0.0355 - val_loss: 0.2903 - val_msle: 0.0490 - learning_rate: 0.0097\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2024 - msle: 0.0327 - val_loss: 0.2775 - val_msle: 0.0463 - learning_rate: 0.0097\n",
      "Epoch 29/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:36:45.388451: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.1884 - msle: 0.0295 - val_loss: 0.2645 - val_msle: 0.0435 - learning_rate: 0.0097\n",
      "Epoch 30/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:37:21.121892: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.1792 - msle: 0.0275 - val_loss: 0.2508 - val_msle: 0.0407 - learning_rate: 0.0097\n",
      "Epoch 31/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:37:57.102736: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.1711 - msle: 0.0260 - val_loss: 0.2376 - val_msle: 0.0380 - learning_rate: 0.0097\n",
      "Epoch 32/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:38:32.992366: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.1637 - msle: 0.0245 - val_loss: 0.2285 - val_msle: 0.0361 - learning_rate: 0.0097\n",
      "Epoch 33/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:39:08.987159: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.1566 - msle: 0.0230 - val_loss: 0.2192 - val_msle: 0.0343 - learning_rate: 0.0097\n",
      "Epoch 34/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.1494 - msle: 0.0215 - val_loss: 0.2110 - val_msle: 0.0327 - learning_rate: 0.0097\n",
      "Epoch 35/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:40:20.977716: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2094 - msle: 0.0345 - val_loss: 0.5088 - val_msle: 0.0972 - learning_rate: 0.0097\n",
      "Epoch 36/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:40:57.379829: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.5938 - msle: 0.1365 - val_loss: 0.5590 - val_msle: 0.1200 - learning_rate: 0.0097\n",
      "Epoch 37/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:41:33.928938: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.8089 - msle: 0.2098 - val_loss: 1.1675 - val_msle: 0.2259 - learning_rate: 0.0096\n",
      "Epoch 38/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:42:10.447927: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 1.2985 - msle: 0.3019 - val_loss: 0.5319 - val_msle: 0.0946 - learning_rate: 0.0096\n",
      "Epoch 39/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.6973 - msle: 0.1391 - val_loss: 0.5273 - val_msle: 0.0901 - learning_rate: 0.0096\n",
      "Epoch 40/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.4403 - msle: 0.0959 - val_loss: 0.4779 - val_msle: 0.0833 - learning_rate: 0.0096\n",
      "Epoch 41/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2723 - msle: 0.0497 - val_loss: 0.4508 - val_msle: 0.0782 - learning_rate: 0.0096\n",
      "Epoch 42/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3817 - msle: 0.0570 - val_loss: 0.6598 - val_msle: 0.0958 - learning_rate: 0.0096\n",
      "Epoch 43/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.5487 - msle: 0.0815 - val_loss: 0.4110 - val_msle: 0.0696 - learning_rate: 0.0096\n",
      "Epoch 44/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:45:48.480172: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.4668 - msle: 0.1175 - val_loss: 0.4767 - val_msle: 0.0773 - learning_rate: 0.0096\n",
      "Epoch 45/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3437 - msle: 0.0631 - val_loss: 0.3994 - val_msle: 0.0618 - learning_rate: 0.0096\n",
      "Epoch 46/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3134 - msle: 0.0615 - val_loss: 0.3891 - val_msle: 0.0623 - learning_rate: 0.0096\n",
      "Epoch 47/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2692 - msle: 0.0478 - val_loss: 0.4114 - val_msle: 0.0643 - learning_rate: 0.0095\n",
      "Epoch 48/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3986 - msle: 0.0737 - val_loss: 0.3231 - val_msle: 0.0532 - learning_rate: 0.0095\n",
      "Epoch 49/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3585 - msle: 0.0737 - val_loss: 0.3111 - val_msle: 0.0509 - learning_rate: 0.0095\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:49:24.091927: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3347 - msle: 0.0635 - val_loss: 0.3457 - val_msle: 0.0540 - learning_rate: 0.0095\n",
      "Epoch 51/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.3339 - msle: 0.0633 - val_loss: 0.3398 - val_msle: 0.0551 - learning_rate: 0.0095\n",
      "Epoch 52/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:50:37.033562: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.3254 - msle: 0.0599 - val_loss: 0.3286 - val_msle: 0.0537 - learning_rate: 0.0095\n",
      "Epoch 53/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:51:13.550612: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3036 - msle: 0.0574 - val_loss: 0.3589 - val_msle: 0.0576 - learning_rate: 0.0095\n",
      "Epoch 54/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:51:49.827346: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2693 - msle: 0.0475 - val_loss: 0.3714 - val_msle: 0.0592 - learning_rate: 0.0095\n",
      "Epoch 55/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:52:26.200420: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2035 - msle: 0.0322 - val_loss: 0.4936 - val_msle: 0.0801 - learning_rate: 0.0095\n",
      "Epoch 56/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:53:02.687717: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3972 - msle: 0.0778 - val_loss: 0.5679 - val_msle: 0.0774 - learning_rate: 0.0095\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:53:38.625341: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.4909 - msle: 0.0767 - val_loss: 0.2886 - val_msle: 0.0472 - learning_rate: 0.0094\n",
      "Epoch 58/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:54:14.430784: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3354 - msle: 0.0623 - val_loss: 0.3934 - val_msle: 0.0592 - learning_rate: 0.0094\n",
      "Epoch 59/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 17:54:50.417338: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2805 - msle: 0.0491 - val_loss: 0.3728 - val_msle: 0.0572 - learning_rate: 0.0094\n",
      "Epoch 60/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2393 - msle: 0.0422 - val_loss: 0.4843 - val_msle: 0.0675 - learning_rate: 0.0094\n",
      "Epoch 61/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3059 - msle: 0.0502 - val_loss: 0.7652 - val_msle: 0.1126 - learning_rate: 0.0094\n",
      "Epoch 62/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3149 - msle: 0.0519 - val_loss: 0.8444 - val_msle: 0.1375 - learning_rate: 0.0094\n",
      "Epoch 63/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.6201 - msle: 0.1260 - val_loss: 0.6399 - val_msle: 0.1209 - learning_rate: 0.0094\n",
      "Epoch 64/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 1.0393 - msle: 0.2900 - val_loss: 0.2388 - val_msle: 0.0383 - learning_rate: 0.0094\n",
      "Epoch 65/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.4079 - msle: 0.0734 - val_loss: 0.4796 - val_msle: 0.0773 - learning_rate: 0.0094\n",
      "Epoch 66/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.4070 - msle: 0.0693 - val_loss: 0.6657 - val_msle: 0.1007 - learning_rate: 0.0094\n",
      "Epoch 67/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.5231 - msle: 0.0874 - val_loss: 0.5601 - val_msle: 0.0872 - learning_rate: 0.0093\n",
      "Epoch 68/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3961 - msle: 0.0696 - val_loss: 0.3309 - val_msle: 0.0526 - learning_rate: 0.0093\n",
      "Epoch 69/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:00:48.606701: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3672 - msle: 0.0696 - val_loss: 0.2721 - val_msle: 0.0461 - learning_rate: 0.0093\n",
      "Epoch 70/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:01:24.429137: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.2917 - msle: 0.0523 - val_loss: 0.3377 - val_msle: 0.0602 - learning_rate: 0.0093\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:02:00.428489: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.2925 - msle: 0.0524 - val_loss: 0.4169 - val_msle: 0.0797 - learning_rate: 0.0093\n",
      "Epoch 72/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:02:37.034992: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 12s/step - loss: 0.3112 - msle: 0.0603 - val_loss: 0.4784 - val_msle: 0.0940 - learning_rate: 0.0093\n",
      "Epoch 73/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.3125 - msle: 0.0612 - val_loss: 0.4924 - val_msle: 0.0936 - learning_rate: 0.0093\n",
      "Epoch 74/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 14s/step - loss: 0.2802 - msle: 0.0523 - val_loss: 0.4534 - val_msle: 0.0849 - learning_rate: 0.0093\n",
      "Epoch 75/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 13s/step - loss: 0.2574 - msle: 0.0472 - val_loss: 0.4109 - val_msle: 0.0736 - learning_rate: 0.0093\n",
      "Epoch 76/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.2288 - msle: 0.0394 - val_loss: 0.4014 - val_msle: 0.0724 - learning_rate: 0.0093\n",
      "Epoch 77/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 18:05:47.607767: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.2126 - msle: 0.0357 - val_loss: 0.3825 - val_msle: 0.0673 - learning_rate: 0.0092\n",
      "Epoch 78/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.1955 - msle: 0.0314 - val_loss: 0.3635 - val_msle: 0.0629 - learning_rate: 0.0092\n",
      "Epoch 79/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.1845 - msle: 0.0291 - val_loss: 0.3471 - val_msle: 0.0598 - learning_rate: 0.0092\n",
      "Epoch 80/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.1739 - msle: 0.0268 - val_loss: 0.3235 - val_msle: 0.0548 - learning_rate: 0.0092\n",
      "Epoch 81/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 12s/step - loss: 0.1647 - msle: 0.0249 - val_loss: 0.3030 - val_msle: 0.0511 - learning_rate: 0.0092\n",
      "Epoch 82/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13s/step - loss: 0.1560 - msle: 0.0231 - val_loss: 0.2850 - val_msle: 0.0480 - learning_rate: 0.0092\n",
      "Epoch 83/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 13s/step - loss: 0.1496 - msle: 0.0219 - val_loss: 0.2737 - val_msle: 0.0457 - learning_rate: 0.0092\n",
      "Epoch 84/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 12s/step - loss: 0.1425 - msle: 0.0205 - val_loss: 0.2597 - val_msle: 0.0433 - learning_rate: 0.0092\n"
     ]
    }
   ],
   "source": [
    "def split_series(series, n_past, n_future):\n",
    "    X, y = [], []\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        X.append(series[window_start:past_end, :])\n",
    "        y.append(series[past_end:future_end, :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_future = 16\n",
    "n_past = scaled_validation_samples.shape[0] - n_future\n",
    "n_features = num_stores_train * num_families_train\n",
    "\n",
    "if (n_past - n_future)%2 == 0:\n",
    "    crop_max = int((n_past - n_future))\n",
    "else:\n",
    "    crop_min = int((n_past - n_future)/2)\n",
    "    crop_max = 2*crop_min + 1\n",
    "\n",
    "X_train, y_train = split_series(scaled_train_samples, n_past, n_future)\n",
    "X_val, y_val = split_series(scaled_validation_samples, n_past, n_future)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same', input_shape=(n_past, n_features)))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.MaxPool1D(strides=1, padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=1024, kernel_size=(15), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=1024, kernel_size=(15), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=1024, kernel_size=(15), padding='same'))\n",
    "model.add(layers.MaxPool1D(strides=1, padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.MaxPool1D(strides=1, padding='same'))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=256, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=256, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Cropping1D(cropping=(0, crop_max)))\n",
    "model.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
    "\n",
    "model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['msle'])\n",
    "model.build((None, n_past, n_features))\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_msle', min_delta=1e-4, patience=50, restore_best_weights=True)\n",
    "\n",
    "model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[lr_cb, early_stopping], \n",
    "                          batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "490023ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:05:06.218684Z",
     "iopub.status.busy": "2024-07-26T08:05:06.218326Z",
     "iopub.status.idle": "2024-07-26T08:05:06.795815Z",
     "shell.execute_reply": "2024-07-26T08:05:06.794869Z"
    },
    "papermill": {
     "duration": 0.748359,
     "end_time": "2024-07-26T08:05:06.797773",
     "exception": false,
     "start_time": "2024-07-26T08:05:06.049414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 588ms/step\n"
     ]
    }
   ],
   "source": [
    "x_test_pred = scaled_validation_samples[-n_past:].reshape((1, n_past, n_features))\n",
    "scaled_y_predict = model.predict(x_test_pred)\n",
    "\n",
    "y_predict = pd.DataFrame(minmax_scaler.inverse_transform(scaled_y_predict.reshape((n_future, n_features))), columns=valid_samples_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "effd2ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:05:07.139459Z",
     "iopub.status.busy": "2024-07-26T08:05:07.138630Z",
     "iopub.status.idle": "2024-07-26T08:05:20.854097Z",
     "shell.execute_reply": "2024-07-26T08:05:20.852995Z"
    },
    "papermill": {
     "duration": 13.887426,
     "end_time": "2024-07-26T08:05:20.856161",
     "exception": false,
     "start_time": "2024-07-26T08:05:06.968735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of the data:\n",
      "               sales\n",
      "id                  \n",
      "3000888     2.882082\n",
      "3000889     0.000000\n",
      "3000890     1.727063\n",
      "3000891  2082.949463\n",
      "3000892     0.930780\n"
     ]
    }
   ],
   "source": [
    "pivoted_test = test_data.pivot(index=['date','dcoilwtico','dayofweek'], columns=['store_nbr', 'family'], values='id')\n",
    "\n",
    "for day_ith, day_ith_pred in y_predict.iterrows():\n",
    "    for n_samples_per_day in range(len(day_ith_pred)):\n",
    "        sample_id = pivoted_test.iloc[[day_ith], [n_samples_per_day]].values[0][0]\n",
    "        values = max(0, day_ith_pred.values[n_samples_per_day])\n",
    "        submission.at[sample_id, 'sales'] = max(0, day_ith_pred.values[n_samples_per_day])\n",
    "\n",
    "submission.to_csv('submission.csv')\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 286.363123,
   "end_time": "2024-07-26T08:05:24.383801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-26T08:00:38.020678",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
