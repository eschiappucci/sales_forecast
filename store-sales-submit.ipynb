{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943b2c20",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-26T08:00:40.913504Z",
     "iopub.status.busy": "2024-07-26T08:00:40.913159Z",
     "iopub.status.idle": "2024-07-26T08:01:00.255565Z",
     "shell.execute_reply": "2024-07-26T08:01:00.254755Z"
    },
    "papermill": {
     "duration": 19.349394,
     "end_time": "2024-07-26T08:01:00.257802",
     "exception": false,
     "start_time": "2024-07-26T08:00:40.908408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 15:09:34.457397: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-06 15:09:34.464693: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-06 15:09:34.514178: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-06 15:09:34.567654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-06 15:09:34.621506: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-06 15:09:34.636720: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-06 15:09:34.727305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-06 15:09:35.722619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_path = 'og_data/'\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates=['date'])\n",
    "test = pd.read_csv(os.path.join(data_path, 'test.csv'), parse_dates=['date'])\n",
    "\n",
    "# holidays_events = pd.read_csv(os.path.join(data_path, 'holidays_events.csv'), parse_dates=['date'])\n",
    "oil = pd.read_csv(os.path.join(data_path, 'oil.csv'), parse_dates=['date'])\n",
    "stores = pd.read_csv(os.path.join(data_path, 'stores.csv'))\n",
    "transactions = pd.read_csv(os.path.join(data_path, 'transactions.csv'), parse_dates=['date'])\n",
    "submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'), index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2756fdd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:01:00.266033Z",
     "iopub.status.busy": "2024-07-26T08:01:00.265752Z",
     "iopub.status.idle": "2024-07-26T08:01:03.324404Z",
     "shell.execute_reply": "2024-07-26T08:01:03.323452Z"
    },
    "papermill": {
     "duration": 3.065145,
     "end_time": "2024-07-26T08:01:03.326557",
     "exception": false,
     "start_time": "2024-07-26T08:01:00.261412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 1506.5585637191425\n",
      "P-value: 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data = pd.read_csv(os.path.join(data_path, 'train.csv'), parse_dates=['date'])\n",
    "\n",
    "data['dayofweek'] = data['date'].dt.dayofweek\n",
    "\n",
    "average_sales = data.groupby('dayofweek')['sales'].mean()\n",
    "\n",
    "average_sales.plot(kind='bar', color='skyblue')\n",
    "plt.title('Average Sales by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xticks(range(0, 7), ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=0)\n",
    "plt.savefig('../weekday.png')\n",
    "\n",
    "grouped = data.groupby('dayofweek')['sales']\n",
    "\n",
    "sales_data = [group for name, group in grouped]\n",
    "\n",
    "f_stat, p_value = f_oneway(*sales_data)\n",
    "print('F-statistic:', f_stat)\n",
    "print('P-value:', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2d88fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_helper(y):\n",
    "    \"\"\"Helper to handle indices and logical indices of NaNs.\n",
    "\n",
    "    Input:\n",
    "        - y, 1d numpy array with possible NaNs\n",
    "    Output:\n",
    "        - nans, logical indices of NaNs\n",
    "        - index, a function, with signature indices= index(logical_indices),\n",
    "          to convert logical indices of NaNs to 'equivalent' indices\n",
    "    Example:\n",
    "        >>> # linear interpolation of NaNs\n",
    "        >>> nans, x= nan_helper(y)\n",
    "        >>> y[nans]= np.interp(x(nans), x(~nans), y[~nans])\n",
    "    \"\"\"\n",
    "\n",
    "    return np.isnan(y), lambda z: z.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a83075",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min, date_max = train.date.min(), test.date.max()\n",
    "oil['date'] = pd.to_datetime(oil['date'])\n",
    "new_df = pd.DataFrame()\n",
    "new_df['date'] = pd.date_range(date_min, date_max)\n",
    "new_df = pd.merge(new_df,oil, how=\"outer\", on='date')\n",
    "new_df['dcoilwtico'] = new_df['dcoilwtico'].interpolate(method = \"linear\")\n",
    "oil = new_df\n",
    "\n",
    "oil_prices = np.asarray(oil['dcoilwtico'])\n",
    "nans, x= nan_helper(oil_prices)\n",
    "oil_prices[nans]= np.interp(x(nans), x(~nans), oil_prices[~nans])\n",
    "oil['dcoilwtico'] = oil_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f340f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            1\n",
       "1            1\n",
       "2            1\n",
       "3            1\n",
       "4            1\n",
       "          ... \n",
       "3000883    227\n",
       "3000884    227\n",
       "3000885    227\n",
       "3000886    227\n",
       "3000887    227\n",
       "Name: date, Length: 3000888, dtype: int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fb9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclicalcolumn(col):\n",
    "    return np.sin((2*np.pi*col)/max(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e1ed22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:01:03.350481Z",
     "iopub.status.busy": "2024-07-26T08:01:03.350216Z",
     "iopub.status.idle": "2024-07-26T08:01:05.696014Z",
     "shell.execute_reply": "2024-07-26T08:01:05.695126Z"
    },
    "papermill": {
     "duration": 2.35348,
     "end_time": "2024-07-26T08:01:05.698750",
     "exception": false,
     "start_time": "2024-07-26T08:01:03.345270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of the original train data:\n",
      "(1599, 1782)\n",
      "store_nbr                                        1                         \\\n",
      "family                                          0    1    2       3    4    \n",
      "date       dcoilwtico dayofweek     dayofyear                               \n",
      "2013-01-01 93.140000   8.660254e-01 0.017166   0.0  0.0  0.0     0.0  0.0   \n",
      "2013-01-02 93.140000   8.660254e-01 0.034328   2.0  0.0  2.0  1091.0  0.0   \n",
      "2013-01-03 92.970000   1.224647e-16 0.051479   3.0  0.0  0.0   919.0  0.0   \n",
      "2013-01-04 93.120000  -8.660254e-01 0.068615   3.0  0.0  3.0   953.0  0.0   \n",
      "2013-01-05 93.146667  -8.660254e-01 0.085731   5.0  0.0  3.0  1160.0  0.0   \n",
      "\n",
      "store_nbr                                                                   \\\n",
      "family                                              5    6       7      8    \n",
      "date       dcoilwtico dayofweek     dayofyear                                \n",
      "2013-01-01 93.140000   8.660254e-01 0.017166     0.000  0.0     0.0    0.0   \n",
      "2013-01-02 93.140000   8.660254e-01 0.034328   470.652  0.0  1060.0  579.0   \n",
      "2013-01-03 92.970000   1.224647e-16 0.051479   310.655  0.0   836.0  453.0   \n",
      "2013-01-04 93.120000  -8.660254e-01 0.068615   198.366  0.0   827.0  460.0   \n",
      "2013-01-05 93.146667  -8.660254e-01 0.085731   301.057  0.0   811.0  464.0   \n",
      "\n",
      "store_nbr                                               ...    9           \\\n",
      "family                                              9   ...   23       24   \n",
      "date       dcoilwtico dayofweek     dayofyear           ...                 \n",
      "2013-01-01 93.140000   8.660254e-01 0.017166     0.000  ...  0.0    0.000   \n",
      "2013-01-02 93.140000   8.660254e-01 0.034328   164.069  ...  0.0  374.531   \n",
      "2013-01-03 92.970000   1.224647e-16 0.051479   151.582  ...  0.0  400.863   \n",
      "2013-01-04 93.120000  -8.660254e-01 0.068615   131.411  ...  0.0  310.878   \n",
      "2013-01-05 93.146667  -8.660254e-01 0.085731   118.613  ...  0.0  447.685   \n",
      "\n",
      "store_nbr                                                                      \\\n",
      "family                                            25   26   27       28    29   \n",
      "date       dcoilwtico dayofweek     dayofyear                                   \n",
      "2013-01-01 93.140000   8.660254e-01 0.017166     0.0  0.0  0.0    0.000   0.0   \n",
      "2013-01-02 93.140000   8.660254e-01 0.034328   482.0  0.0  0.0  651.292  83.0   \n",
      "2013-01-03 92.970000   1.224647e-16 0.051479   372.0  0.0  0.0  509.496  66.0   \n",
      "2013-01-04 93.120000  -8.660254e-01 0.068615   324.0  0.0  0.0  332.672  57.0   \n",
      "2013-01-05 93.146667  -8.660254e-01 0.085731   461.0  0.0  0.0  510.919  84.0   \n",
      "\n",
      "store_nbr                                                        \n",
      "family                                          30   31      32  \n",
      "date       dcoilwtico dayofweek     dayofyear                    \n",
      "2013-01-01 93.140000   8.660254e-01 0.017166   0.0  0.0   0.000  \n",
      "2013-01-02 93.140000   8.660254e-01 0.034328   0.0  0.0  29.214  \n",
      "2013-01-03 92.970000   1.224647e-16 0.051479   0.0  0.0  25.000  \n",
      "2013-01-04 93.120000  -8.660254e-01 0.068615   0.0  0.0  11.000  \n",
      "2013-01-05 93.146667  -8.660254e-01 0.085731   0.0  0.0  28.361  \n",
      "\n",
      "[5 rows x 1782 columns]\n",
      "\n",
      "First 5 rows of the original train data:\n",
      "(1599, 1782)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.11111111 0.         0.16666667 ... 0.         0.         0.48512928]\n",
      " [0.16666667 0.         0.         ... 0.         0.         0.41515136]\n",
      " ...\n",
      " [0.22222222 0.         0.25       ... 0.49480247 0.00920245 0.45467379]\n",
      " [0.05555556 0.         0.08333333 ... 0.59724127 0.02147239 0.65942643]\n",
      " [0.11111111 0.         0.41666667 ... 0.34804615 0.00613497 0.31712582]]\n"
     ]
    }
   ],
   "source": [
    "train_data = train.copy().drop(['onpromotion'], axis=1)\n",
    "test_data = test.copy().drop(['onpromotion'], axis=1)\n",
    "\n",
    "train_data['dayofweek'] = cyclicalcolumn(data['date'].dt.dayofweek)\n",
    "test_data['dayofweek'] = cyclicalcolumn(data['date'].dt.dayofweek)\n",
    "\n",
    "train_data['dayofyear'] = cyclicalcolumn(data['date'].dt.dayofyear)\n",
    "test_data['dayofyear'] = cyclicalcolumn(data['date'].dt.dayofyear)\n",
    "\n",
    "train_data = train_data.merge(oil, on='date', how='left')\n",
    "test_data = test_data.merge(oil, on='date', how='left')\n",
    "\n",
    "# Ordinal Encoding for 'family' column\n",
    "ordinal_encoder = OrdinalEncoder(dtype=int)\n",
    "train_data[['family']] = ordinal_encoder.fit_transform(train_data[['family']])\n",
    "test_data[['family']] = ordinal_encoder.transform(test_data[['family']])\n",
    "\n",
    "num_days_train = train[\"date\"].nunique()\n",
    "num_stores_train = train[\"store_nbr\"].nunique()\n",
    "num_families_train = train[\"family\"].nunique()\n",
    "\n",
    "num_days_test = test[\"date\"].nunique()\n",
    "num_stores_test = test[\"store_nbr\"].nunique()\n",
    "num_families_test = test[\"family\"].nunique()\n",
    "\n",
    "pivoted_train = train_data.pivot(index=['date','dcoilwtico','dayofweek','dayofyear'], columns=['store_nbr', 'family'], values='sales')\n",
    "\n",
    "train_samples = int(num_days_train * 0.95)\n",
    "train_samples_df = pivoted_train[:train_samples]\n",
    "valid_samples_df = pivoted_train[train_samples:]\n",
    "\n",
    "print(\"\\nFirst 5 rows of the original train data:\")\n",
    "print(train_samples_df.shape)\n",
    "print(train_samples_df.head())\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler.fit(train_samples_df)\n",
    "scaled_train_samples = minmax_scaler.transform(train_samples_df)\n",
    "scaled_validation_samples = minmax_scaler.transform(valid_samples_df)\n",
    "\n",
    "print(\"\\nFirst 5 rows of the original train data:\")\n",
    "print(scaled_train_samples.shape)\n",
    "print(scaled_train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccd28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, count, max_var = 0, 0, 20\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 1e-2, 1e-5 * batch_size, 1e-4\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 0, 0, 0.75\n",
    "    line_max, line_min = 1, 1e-2\n",
    "    \n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'line': \n",
    "            m = (lr_min - lr_start)/epochs\n",
    "            lr = m * epoch + lr_start\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = (epochs - lr_ramp_ep - lr_sus_ep) /(epochs/20), epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            m = (line_min - line_max)/epochs\n",
    "            control = m * epoch + line_max\n",
    "            lr = ((lr_start - lr_min) * (1+np.cos(phase)) + lr_min) * control\n",
    "        elif mode == 'var':\n",
    "            global a, count, max_var\n",
    "            m = (lr_min - lr_start)/max_var\n",
    "            if a < max_var:\n",
    "                lr = m * (epoch-count) + lr_start\n",
    "                a += 1\n",
    "            elif a >= max_var:\n",
    "                print(epoch-count)\n",
    "                lr = m * (epoch-count) + lr_start\n",
    "                count += max_var + 1\n",
    "                a = 0\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149d8112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "lr_mode = 'line'\n",
    "epochs = 1000 \n",
    "lr_cb = get_lr_callback(batch_size, mode=lr_mode, epochs=epochs, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad32ef96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:01:05.709144Z",
     "iopub.status.busy": "2024-07-26T08:01:05.708458Z",
     "iopub.status.idle": "2024-07-26T08:05:05.541386Z",
     "shell.execute_reply": "2024-07-26T08:05:05.540347Z"
    },
    "papermill": {
     "duration": 239.840818,
     "end_time": "2024-07-26T08:05:05.543883",
     "exception": false,
     "start_time": "2024-07-26T08:01:05.703065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ separable_conv1d                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">925,370</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">266,240</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">266,240</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">532,992</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,064,960</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_5              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,064,960</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_6              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">531,968</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_7              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">266,240</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_8              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">266,240</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv1D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,574,912</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cropping1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cropping1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1782</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">914,166</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ separable_conv1d                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m925,370\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m266,240\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m266,240\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │       \u001b[38;5;34m532,992\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m1,064,960\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_5              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m1,064,960\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_6              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m531,968\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_7              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m266,240\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ separable_conv1d_8              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m266,240\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv1D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m1,574,912\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m69\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ cropping1d (\u001b[38;5;33mCropping1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m1782\u001b[0m)       │       \u001b[38;5;34m914,166\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,253,296</span> (35.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,253,296\u001b[0m (35.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,251,248</span> (35.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,251,248\u001b[0m (35.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> (8.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,048\u001b[0m (8.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 10s/step - loss: 0.3601 - msle: 0.0743 - val_loss: 0.2116 - val_msle: 0.0393 - learning_rate: 0.0100\n",
      "Epoch 2/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 1.0948 - msle: 0.2403 - val_loss: 0.6091 - val_msle: 0.0999 - learning_rate: 0.0100\n",
      "Epoch 3/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 1.3700 - msle: 0.3730 - val_loss: 0.6297 - val_msle: 0.0920 - learning_rate: 0.0100\n",
      "Epoch 4/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 9s/step - loss: 0.5253 - msle: 0.1016 - val_loss: 0.5493 - val_msle: 0.0858 - learning_rate: 0.0100\n",
      "Epoch 5/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 15:11:58.716629: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.9142 - msle: 0.1749 - val_loss: 0.5451 - val_msle: 0.1059 - learning_rate: 0.0100\n",
      "Epoch 6/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.7212 - msle: 0.1538 - val_loss: 0.7718 - val_msle: 0.1767 - learning_rate: 0.0100\n",
      "Epoch 7/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 15:12:56.923575: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.6833 - msle: 0.1601 - val_loss: 0.7089 - val_msle: 0.1026 - learning_rate: 0.0099\n",
      "Epoch 8/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.4987 - msle: 0.0775 - val_loss: 0.9450 - val_msle: 0.1424 - learning_rate: 0.0099\n",
      "Epoch 9/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.4061 - msle: 0.0864 - val_loss: 0.8446 - val_msle: 0.1215 - learning_rate: 0.0099\n",
      "Epoch 10/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 15:14:27.450937: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.3288 - msle: 0.0638 - val_loss: 0.7670 - val_msle: 0.1104 - learning_rate: 0.0099\n",
      "Epoch 11/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.3172 - msle: 0.0572 - val_loss: 0.4491 - val_msle: 0.0708 - learning_rate: 0.0099\n",
      "Epoch 12/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.3116 - msle: 0.0676 - val_loss: 0.4928 - val_msle: 0.1083 - learning_rate: 0.0099\n",
      "Epoch 13/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.2581 - msle: 0.0468 - val_loss: 0.4367 - val_msle: 0.0931 - learning_rate: 0.0099\n",
      "Epoch 14/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.2493 - msle: 0.0487 - val_loss: 0.4171 - val_msle: 0.0885 - learning_rate: 0.0099\n",
      "Epoch 15/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.2174 - msle: 0.0389 - val_loss: 0.4299 - val_msle: 0.0915 - learning_rate: 0.0099\n",
      "Epoch 16/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1763 - msle: 0.0329 - val_loss: 0.4398 - val_msle: 0.0940 - learning_rate: 0.0099\n",
      "Epoch 17/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.1525 - msle: 0.0251 - val_loss: 0.4314 - val_msle: 0.0915 - learning_rate: 0.0098\n",
      "Epoch 18/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1341 - msle: 0.0207 - val_loss: 0.4249 - val_msle: 0.0887 - learning_rate: 0.0098\n",
      "Epoch 19/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.1257 - msle: 0.0192 - val_loss: 0.4248 - val_msle: 0.0873 - learning_rate: 0.0098\n",
      "Epoch 20/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 15:19:29.957233: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.1174 - msle: 0.0180 - val_loss: 0.4278 - val_msle: 0.0886 - learning_rate: 0.0098\n",
      "Epoch 21/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.1147 - msle: 0.0166 - val_loss: 0.4218 - val_msle: 0.0872 - learning_rate: 0.0098\n",
      "Epoch 22/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.1107 - msle: 0.0162 - val_loss: 0.4168 - val_msle: 0.0856 - learning_rate: 0.0098\n",
      "Epoch 23/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.1069 - msle: 0.0156 - val_loss: 0.4161 - val_msle: 0.0852 - learning_rate: 0.0098\n",
      "Epoch 24/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.1039 - msle: 0.0147 - val_loss: 0.4103 - val_msle: 0.0834 - learning_rate: 0.0098\n",
      "Epoch 25/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.1009 - msle: 0.0144 - val_loss: 0.4078 - val_msle: 0.0829 - learning_rate: 0.0098\n",
      "Epoch 26/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0988 - msle: 0.0138 - val_loss: 0.4029 - val_msle: 0.0816 - learning_rate: 0.0098\n",
      "Epoch 27/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.0968 - msle: 0.0134 - val_loss: 0.3967 - val_msle: 0.0797 - learning_rate: 0.0097\n",
      "Epoch 28/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0948 - msle: 0.0130 - val_loss: 0.3922 - val_msle: 0.0781 - learning_rate: 0.0097\n",
      "Epoch 29/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.0930 - msle: 0.0126 - val_loss: 0.3847 - val_msle: 0.0759 - learning_rate: 0.0097\n",
      "Epoch 30/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.0914 - msle: 0.0124 - val_loss: 0.3792 - val_msle: 0.0744 - learning_rate: 0.0097\n",
      "Epoch 31/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0899 - msle: 0.0120 - val_loss: 0.3748 - val_msle: 0.0732 - learning_rate: 0.0097\n",
      "Epoch 32/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.0886 - msle: 0.0118 - val_loss: 0.3693 - val_msle: 0.0717 - learning_rate: 0.0097\n",
      "Epoch 33/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.0878 - msle: 0.0117 - val_loss: 0.3646 - val_msle: 0.0704 - learning_rate: 0.0097\n",
      "Epoch 34/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0869 - msle: 0.0116 - val_loss: 0.3596 - val_msle: 0.0690 - learning_rate: 0.0097\n",
      "Epoch 35/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0862 - msle: 0.0115 - val_loss: 0.3548 - val_msle: 0.0677 - learning_rate: 0.0097\n",
      "Epoch 36/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0859 - msle: 0.0114 - val_loss: 0.3501 - val_msle: 0.0665 - learning_rate: 0.0097\n",
      "Epoch 37/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0859 - msle: 0.0114 - val_loss: 0.3457 - val_msle: 0.0653 - learning_rate: 0.0096\n",
      "Epoch 38/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 10s/step - loss: 0.0865 - msle: 0.0115 - val_loss: 0.3246 - val_msle: 0.0599 - learning_rate: 0.0096\n",
      "Epoch 39/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 15:28:25.964670: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 310210560 bytes after encountering the first element of size 310210560 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.3176 - msle: 0.0701 - val_loss: 0.2462 - val_msle: 0.0391 - learning_rate: 0.0096\n",
      "Epoch 40/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.5700 - msle: 0.1186 - val_loss: 0.2610 - val_msle: 0.0449 - learning_rate: 0.0096\n",
      "Epoch 41/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 9s/step - loss: 0.3917 - msle: 0.0695 - val_loss: 0.2465 - val_msle: 0.0384 - learning_rate: 0.0096\n",
      "Epoch 42/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.2914 - msle: 0.0473 - val_loss: 0.2992 - val_msle: 0.0467 - learning_rate: 0.0096\n",
      "Epoch 43/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.2349 - msle: 0.0388 - val_loss: 0.2596 - val_msle: 0.0422 - learning_rate: 0.0096\n",
      "Epoch 44/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9s/step - loss: 0.2222 - msle: 0.0368 - val_loss: 0.3053 - val_msle: 0.0507 - learning_rate: 0.0096\n",
      "Epoch 45/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.2006 - msle: 0.0321 - val_loss: 0.2708 - val_msle: 0.0435 - learning_rate: 0.0096\n",
      "Epoch 46/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1814 - msle: 0.0284 - val_loss: 0.2602 - val_msle: 0.0430 - learning_rate: 0.0096\n",
      "Epoch 47/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1583 - msle: 0.0241 - val_loss: 0.2832 - val_msle: 0.0490 - learning_rate: 0.0095\n",
      "Epoch 48/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1428 - msle: 0.0211 - val_loss: 0.2556 - val_msle: 0.0431 - learning_rate: 0.0095\n",
      "Epoch 49/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1338 - msle: 0.0187 - val_loss: 0.2470 - val_msle: 0.0406 - learning_rate: 0.0095\n",
      "Epoch 50/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.1231 - msle: 0.0161 - val_loss: 0.2252 - val_msle: 0.0370 - learning_rate: 0.0095\n",
      "Epoch 51/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.1156 - msle: 0.0149 - val_loss: 0.2221 - val_msle: 0.0359 - learning_rate: 0.0095\n",
      "Epoch 52/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.1113 - msle: 0.0141 - val_loss: 0.2238 - val_msle: 0.0360 - learning_rate: 0.0095\n",
      "Epoch 53/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1053 - msle: 0.0125 - val_loss: 0.2130 - val_msle: 0.0338 - learning_rate: 0.0095\n",
      "Epoch 54/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.1001 - msle: 0.0118 - val_loss: 0.2055 - val_msle: 0.0325 - learning_rate: 0.0095\n",
      "Epoch 55/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0976 - msle: 0.0114 - val_loss: 0.2068 - val_msle: 0.0326 - learning_rate: 0.0095\n",
      "Epoch 56/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0945 - msle: 0.0107 - val_loss: 0.2002 - val_msle: 0.0317 - learning_rate: 0.0095\n",
      "Epoch 57/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0921 - msle: 0.0103 - val_loss: 0.2054 - val_msle: 0.0326 - learning_rate: 0.0094\n",
      "Epoch 58/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0902 - msle: 0.0101 - val_loss: 0.2003 - val_msle: 0.0315 - learning_rate: 0.0094\n",
      "Epoch 59/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0888 - msle: 0.0098 - val_loss: 0.2019 - val_msle: 0.0317 - learning_rate: 0.0094\n",
      "Epoch 60/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0875 - msle: 0.0096 - val_loss: 0.1961 - val_msle: 0.0312 - learning_rate: 0.0094\n",
      "Epoch 61/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0856 - msle: 0.0093 - val_loss: 0.1936 - val_msle: 0.0306 - learning_rate: 0.0094\n",
      "Epoch 62/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0836 - msle: 0.0090 - val_loss: 0.1991 - val_msle: 0.0312 - learning_rate: 0.0094\n",
      "Epoch 63/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0824 - msle: 0.0089 - val_loss: 0.1973 - val_msle: 0.0311 - learning_rate: 0.0094\n",
      "Epoch 64/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0810 - msle: 0.0086 - val_loss: 0.1867 - val_msle: 0.0295 - learning_rate: 0.0094\n",
      "Epoch 65/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0804 - msle: 0.0086 - val_loss: 0.1872 - val_msle: 0.0298 - learning_rate: 0.0094\n",
      "Epoch 66/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0786 - msle: 0.0083 - val_loss: 0.1795 - val_msle: 0.0286 - learning_rate: 0.0094\n",
      "Epoch 67/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0780 - msle: 0.0083 - val_loss: 0.1762 - val_msle: 0.0279 - learning_rate: 0.0093\n",
      "Epoch 68/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0768 - msle: 0.0081 - val_loss: 0.1738 - val_msle: 0.0279 - learning_rate: 0.0093\n",
      "Epoch 69/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0757 - msle: 0.0080 - val_loss: 0.1673 - val_msle: 0.0267 - learning_rate: 0.0093\n",
      "Epoch 70/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0749 - msle: 0.0079 - val_loss: 0.1708 - val_msle: 0.0273 - learning_rate: 0.0093\n",
      "Epoch 71/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9s/step - loss: 0.0746 - msle: 0.0078 - val_loss: 0.1611 - val_msle: 0.0257 - learning_rate: 0.0093\n",
      "Epoch 72/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0739 - msle: 0.0079 - val_loss: 0.1640 - val_msle: 0.0262 - learning_rate: 0.0093\n",
      "Epoch 73/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0728 - msle: 0.0076 - val_loss: 0.1575 - val_msle: 0.0256 - learning_rate: 0.0093\n",
      "Epoch 74/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0723 - msle: 0.0076 - val_loss: 0.1580 - val_msle: 0.0254 - learning_rate: 0.0093\n",
      "Epoch 75/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0717 - msle: 0.0075 - val_loss: 0.1521 - val_msle: 0.0245 - learning_rate: 0.0093\n",
      "Epoch 76/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0714 - msle: 0.0076 - val_loss: 0.1494 - val_msle: 0.0240 - learning_rate: 0.0093\n",
      "Epoch 77/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0724 - msle: 0.0077 - val_loss: 0.1510 - val_msle: 0.0247 - learning_rate: 0.0092\n",
      "Epoch 78/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0715 - msle: 0.0076 - val_loss: 0.1525 - val_msle: 0.0246 - learning_rate: 0.0092\n",
      "Epoch 79/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0702 - msle: 0.0073 - val_loss: 0.1409 - val_msle: 0.0230 - learning_rate: 0.0092\n",
      "Epoch 80/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0711 - msle: 0.0075 - val_loss: 0.1513 - val_msle: 0.0246 - learning_rate: 0.0092\n",
      "Epoch 81/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0705 - msle: 0.0073 - val_loss: 0.1394 - val_msle: 0.0229 - learning_rate: 0.0092\n",
      "Epoch 82/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0689 - msle: 0.0073 - val_loss: 0.1480 - val_msle: 0.0236 - learning_rate: 0.0092\n",
      "Epoch 83/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0688 - msle: 0.0073 - val_loss: 0.1460 - val_msle: 0.0244 - learning_rate: 0.0092\n",
      "Epoch 84/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0681 - msle: 0.0071 - val_loss: 0.1439 - val_msle: 0.0233 - learning_rate: 0.0092\n",
      "Epoch 85/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0677 - msle: 0.0071 - val_loss: 0.1466 - val_msle: 0.0249 - learning_rate: 0.0092\n",
      "Epoch 86/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0673 - msle: 0.0071 - val_loss: 0.1409 - val_msle: 0.0227 - learning_rate: 0.0092\n",
      "Epoch 87/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0671 - msle: 0.0071 - val_loss: 0.1412 - val_msle: 0.0235 - learning_rate: 0.0091\n",
      "Epoch 88/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0668 - msle: 0.0070 - val_loss: 0.1384 - val_msle: 0.0223 - learning_rate: 0.0091\n",
      "Epoch 89/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0664 - msle: 0.0070 - val_loss: 0.1377 - val_msle: 0.0229 - learning_rate: 0.0091\n",
      "Epoch 90/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0659 - msle: 0.0070 - val_loss: 0.1364 - val_msle: 0.0220 - learning_rate: 0.0091\n",
      "Epoch 91/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0656 - msle: 0.0070 - val_loss: 0.1370 - val_msle: 0.0226 - learning_rate: 0.0091\n",
      "Epoch 92/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0655 - msle: 0.0069 - val_loss: 0.1350 - val_msle: 0.0218 - learning_rate: 0.0091\n",
      "Epoch 93/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0654 - msle: 0.0069 - val_loss: 0.1351 - val_msle: 0.0223 - learning_rate: 0.0091\n",
      "Epoch 94/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0651 - msle: 0.0069 - val_loss: 0.1723 - val_msle: 0.0309 - learning_rate: 0.0091\n",
      "Epoch 95/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9s/step - loss: 0.1020 - msle: 0.0173 - val_loss: 0.1879 - val_msle: 0.0281 - learning_rate: 0.0091\n",
      "Epoch 96/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 11s/step - loss: 0.0792 - msle: 0.0101 - val_loss: 0.2444 - val_msle: 0.0378 - learning_rate: 0.0091\n",
      "Epoch 97/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0903 - msle: 0.0106 - val_loss: 0.3096 - val_msle: 0.0564 - learning_rate: 0.0090\n",
      "Epoch 98/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 10s/step - loss: 0.0944 - msle: 0.0122 - val_loss: 0.3402 - val_msle: 0.0671 - learning_rate: 0.0090\n",
      "Epoch 99/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0861 - msle: 0.0109 - val_loss: 0.3533 - val_msle: 0.0725 - learning_rate: 0.0090\n",
      "Epoch 100/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0896 - msle: 0.0115 - val_loss: 0.3533 - val_msle: 0.0713 - learning_rate: 0.0090\n",
      "Epoch 101/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0939 - msle: 0.0122 - val_loss: 0.2771 - val_msle: 0.0497 - learning_rate: 0.0090\n",
      "Epoch 102/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0990 - msle: 0.0131 - val_loss: 0.2211 - val_msle: 0.0374 - learning_rate: 0.0090\n",
      "Epoch 103/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0968 - msle: 0.0129 - val_loss: 0.1582 - val_msle: 0.0238 - learning_rate: 0.0090\n",
      "Epoch 104/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0932 - msle: 0.0122 - val_loss: 0.1253 - val_msle: 0.0187 - learning_rate: 0.0090\n",
      "Epoch 105/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0930 - msle: 0.0121 - val_loss: 0.1082 - val_msle: 0.0154 - learning_rate: 0.0090\n",
      "Epoch 106/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0909 - msle: 0.0117 - val_loss: 0.1048 - val_msle: 0.0149 - learning_rate: 0.0090\n",
      "Epoch 107/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0924 - msle: 0.0119 - val_loss: 0.1078 - val_msle: 0.0157 - learning_rate: 0.0090\n",
      "Epoch 108/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0891 - msle: 0.0113 - val_loss: 0.1035 - val_msle: 0.0152 - learning_rate: 0.0089\n",
      "Epoch 109/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0895 - msle: 0.0114 - val_loss: 0.1020 - val_msle: 0.0152 - learning_rate: 0.0089\n",
      "Epoch 110/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.1034 - msle: 0.0152 - val_loss: 0.2717 - val_msle: 0.0539 - learning_rate: 0.0089\n",
      "Epoch 111/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.1377 - msle: 0.0260 - val_loss: 0.1491 - val_msle: 0.0221 - learning_rate: 0.0089\n",
      "Epoch 112/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.1059 - msle: 0.0136 - val_loss: 0.1281 - val_msle: 0.0187 - learning_rate: 0.0089\n",
      "Epoch 113/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0981 - msle: 0.0132 - val_loss: 0.1169 - val_msle: 0.0163 - learning_rate: 0.0089\n",
      "Epoch 114/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0932 - msle: 0.0122 - val_loss: 0.1200 - val_msle: 0.0180 - learning_rate: 0.0089\n",
      "Epoch 115/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0909 - msle: 0.0117 - val_loss: 0.1226 - val_msle: 0.0171 - learning_rate: 0.0089\n",
      "Epoch 116/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0881 - msle: 0.0115 - val_loss: 0.1289 - val_msle: 0.0173 - learning_rate: 0.0089\n",
      "Epoch 117/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0860 - msle: 0.0112 - val_loss: 0.1334 - val_msle: 0.0177 - learning_rate: 0.0089\n",
      "Epoch 118/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0839 - msle: 0.0108 - val_loss: 0.1373 - val_msle: 0.0182 - learning_rate: 0.0088\n",
      "Epoch 119/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0840 - msle: 0.0110 - val_loss: 0.1392 - val_msle: 0.0184 - learning_rate: 0.0088\n",
      "Epoch 120/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0830 - msle: 0.0110 - val_loss: 0.1428 - val_msle: 0.0186 - learning_rate: 0.0088\n",
      "Epoch 121/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0820 - msle: 0.0108 - val_loss: 0.1485 - val_msle: 0.0191 - learning_rate: 0.0088\n",
      "Epoch 122/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0818 - msle: 0.0108 - val_loss: 0.1486 - val_msle: 0.0192 - learning_rate: 0.0088\n",
      "Epoch 123/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11s/step - loss: 0.0814 - msle: 0.0107 - val_loss: 0.1474 - val_msle: 0.0190 - learning_rate: 0.0088\n",
      "Epoch 124/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0832 - msle: 0.0107 - val_loss: 0.2882 - val_msle: 0.0482 - learning_rate: 0.0088\n",
      "Epoch 125/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0862 - msle: 0.0114 - val_loss: 0.4188 - val_msle: 0.0687 - learning_rate: 0.0088\n",
      "Epoch 126/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.1090 - msle: 0.0134 - val_loss: 0.3439 - val_msle: 0.0578 - learning_rate: 0.0088\n",
      "Epoch 127/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.1948 - msle: 0.0239 - val_loss: 0.1810 - val_msle: 0.0286 - learning_rate: 0.0088\n",
      "Epoch 128/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.1532 - msle: 0.0275 - val_loss: 0.1535 - val_msle: 0.0245 - learning_rate: 0.0087\n",
      "Epoch 129/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 12s/step - loss: 0.1065 - msle: 0.0145 - val_loss: 0.1395 - val_msle: 0.0205 - learning_rate: 0.0087\n",
      "Epoch 130/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 0.1089 - msle: 0.0137 - val_loss: 0.1298 - val_msle: 0.0193 - learning_rate: 0.0087\n",
      "Epoch 131/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11s/step - loss: 0.1025 - msle: 0.0135 - val_loss: 0.1237 - val_msle: 0.0180 - learning_rate: 0.0087\n",
      "Epoch 132/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 11s/step - loss: 0.0978 - msle: 0.0130 - val_loss: 0.1176 - val_msle: 0.0173 - learning_rate: 0.0087\n",
      "Epoch 133/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 0.0969 - msle: 0.0126 - val_loss: 0.1201 - val_msle: 0.0180 - learning_rate: 0.0087\n",
      "Epoch 134/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11s/step - loss: 0.0910 - msle: 0.0117 - val_loss: 0.1235 - val_msle: 0.0189 - learning_rate: 0.0087\n",
      "Epoch 135/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 0.0887 - msle: 0.0114 - val_loss: 0.1291 - val_msle: 0.0196 - learning_rate: 0.0087\n",
      "Epoch 136/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0879 - msle: 0.0114 - val_loss: 0.1340 - val_msle: 0.0204 - learning_rate: 0.0087\n",
      "Epoch 137/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0865 - msle: 0.0113 - val_loss: 0.1365 - val_msle: 0.0210 - learning_rate: 0.0087\n",
      "Epoch 138/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11s/step - loss: 0.0851 - msle: 0.0111 - val_loss: 0.1389 - val_msle: 0.0216 - learning_rate: 0.0086\n",
      "Epoch 139/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0842 - msle: 0.0109 - val_loss: 0.1385 - val_msle: 0.0219 - learning_rate: 0.0086\n",
      "Epoch 140/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11s/step - loss: 0.0834 - msle: 0.0109 - val_loss: 0.1328 - val_msle: 0.0213 - learning_rate: 0.0086\n",
      "Epoch 141/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 0.0830 - msle: 0.0109 - val_loss: 0.1304 - val_msle: 0.0211 - learning_rate: 0.0086\n",
      "Epoch 142/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 11s/step - loss: 0.0824 - msle: 0.0108 - val_loss: 0.1461 - val_msle: 0.0234 - learning_rate: 0.0086\n",
      "Epoch 143/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0821 - msle: 0.0108 - val_loss: 0.1481 - val_msle: 0.0238 - learning_rate: 0.0086\n",
      "Epoch 144/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 0.0817 - msle: 0.0107 - val_loss: 0.1497 - val_msle: 0.0241 - learning_rate: 0.0086\n",
      "Epoch 145/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 12s/step - loss: 0.0814 - msle: 0.0107 - val_loss: 0.1511 - val_msle: 0.0244 - learning_rate: 0.0086\n",
      "Epoch 146/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0813 - msle: 0.0107 - val_loss: 0.1519 - val_msle: 0.0246 - learning_rate: 0.0086\n",
      "Epoch 147/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0810 - msle: 0.0107 - val_loss: 0.1530 - val_msle: 0.0248 - learning_rate: 0.0086\n",
      "Epoch 148/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0808 - msle: 0.0107 - val_loss: 0.1542 - val_msle: 0.0250 - learning_rate: 0.0085\n",
      "Epoch 149/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0808 - msle: 0.0107 - val_loss: 0.1561 - val_msle: 0.0253 - learning_rate: 0.0085\n",
      "Epoch 150/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0807 - msle: 0.0107 - val_loss: 0.1576 - val_msle: 0.0256 - learning_rate: 0.0085\n",
      "Epoch 151/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0805 - msle: 0.0107 - val_loss: 0.1593 - val_msle: 0.0258 - learning_rate: 0.0085\n",
      "Epoch 152/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0804 - msle: 0.0107 - val_loss: 0.1606 - val_msle: 0.0260 - learning_rate: 0.0085\n",
      "Epoch 153/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0803 - msle: 0.0107 - val_loss: 0.1613 - val_msle: 0.0261 - learning_rate: 0.0085\n",
      "Epoch 154/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0803 - msle: 0.0106 - val_loss: 0.1575 - val_msle: 0.0248 - learning_rate: 0.0085\n",
      "Epoch 155/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0896 - msle: 0.0124 - val_loss: 0.1568 - val_msle: 0.0224 - learning_rate: 0.0085\n",
      "Epoch 156/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0820 - msle: 0.0100 - val_loss: 0.1518 - val_msle: 0.0219 - learning_rate: 0.0085\n",
      "Epoch 157/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0809 - msle: 0.0100 - val_loss: 0.1541 - val_msle: 0.0222 - learning_rate: 0.0085\n",
      "Epoch 158/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0816 - msle: 0.0104 - val_loss: 0.1584 - val_msle: 0.0232 - learning_rate: 0.0084\n",
      "Epoch 159/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0823 - msle: 0.0107 - val_loss: 0.1712 - val_msle: 0.0254 - learning_rate: 0.0084\n",
      "Epoch 160/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0821 - msle: 0.0109 - val_loss: 0.1672 - val_msle: 0.0250 - learning_rate: 0.0084\n",
      "Epoch 161/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 10s/step - loss: 0.0816 - msle: 0.0108 - val_loss: 0.1664 - val_msle: 0.0249 - learning_rate: 0.0084\n",
      "Epoch 162/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0808 - msle: 0.0107 - val_loss: 0.1715 - val_msle: 0.0257 - learning_rate: 0.0084\n",
      "Epoch 163/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0803 - msle: 0.0106 - val_loss: 0.1712 - val_msle: 0.0255 - learning_rate: 0.0084\n",
      "Epoch 164/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0803 - msle: 0.0106 - val_loss: 0.1691 - val_msle: 0.0252 - learning_rate: 0.0084\n",
      "Epoch 165/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0802 - msle: 0.0106 - val_loss: 0.1628 - val_msle: 0.0242 - learning_rate: 0.0084\n",
      "Epoch 166/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0800 - msle: 0.0105 - val_loss: 0.1151 - val_msle: 0.0192 - learning_rate: 0.0084\n",
      "Epoch 167/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0804 - msle: 0.0106 - val_loss: 0.1126 - val_msle: 0.0183 - learning_rate: 0.0084\n",
      "Epoch 168/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0804 - msle: 0.0106 - val_loss: 0.1160 - val_msle: 0.0175 - learning_rate: 0.0083\n",
      "Epoch 169/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 11s/step - loss: 0.0799 - msle: 0.0105 - val_loss: 0.1096 - val_msle: 0.0186 - learning_rate: 0.0083\n",
      "Epoch 170/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0802 - msle: 0.0106 - val_loss: 0.1107 - val_msle: 0.0188 - learning_rate: 0.0083\n",
      "Epoch 171/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0800 - msle: 0.0106 - val_loss: 0.1110 - val_msle: 0.0191 - learning_rate: 0.0083\n",
      "Epoch 172/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0868 - msle: 0.0130 - val_loss: 0.1089 - val_msle: 0.0181 - learning_rate: 0.0083\n",
      "Epoch 173/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.1348 - msle: 0.0301 - val_loss: 0.1229 - val_msle: 0.0198 - learning_rate: 0.0083\n",
      "Epoch 174/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0973 - msle: 0.0128 - val_loss: 0.1093 - val_msle: 0.0188 - learning_rate: 0.0083\n",
      "Epoch 175/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0877 - msle: 0.0115 - val_loss: 0.1103 - val_msle: 0.0191 - learning_rate: 0.0083\n",
      "Epoch 176/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 11s/step - loss: 0.0843 - msle: 0.0109 - val_loss: 0.1127 - val_msle: 0.0190 - learning_rate: 0.0083\n",
      "Epoch 177/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0816 - msle: 0.0106 - val_loss: 0.1138 - val_msle: 0.0190 - learning_rate: 0.0083\n",
      "Epoch 178/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0842 - msle: 0.0107 - val_loss: 0.1163 - val_msle: 0.0186 - learning_rate: 0.0082\n",
      "Epoch 179/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0853 - msle: 0.0104 - val_loss: 0.1283 - val_msle: 0.0206 - learning_rate: 0.0082\n",
      "Epoch 180/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 9s/step - loss: 0.0850 - msle: 0.0109 - val_loss: 0.1207 - val_msle: 0.0182 - learning_rate: 0.0082\n",
      "Epoch 181/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 9s/step - loss: 0.0824 - msle: 0.0106 - val_loss: 0.1236 - val_msle: 0.0187 - learning_rate: 0.0082\n",
      "Epoch 182/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0821 - msle: 0.0107 - val_loss: 0.3635 - val_msle: 0.0553 - learning_rate: 0.0082\n",
      "Epoch 183/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 10s/step - loss: 0.0814 - msle: 0.0106 - val_loss: 0.3293 - val_msle: 0.0511 - learning_rate: 0.0082\n",
      "Epoch 184/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 10s/step - loss: 0.0812 - msle: 0.0106 - val_loss: 0.3031 - val_msle: 0.0471 - learning_rate: 0.0082\n",
      "Epoch 185/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0806 - msle: 0.0106 - val_loss: 0.2785 - val_msle: 0.0433 - learning_rate: 0.0082\n",
      "Epoch 186/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 11s/step - loss: 0.0807 - msle: 0.0107 - val_loss: 0.2603 - val_msle: 0.0405 - learning_rate: 0.0082\n",
      "Epoch 187/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0801 - msle: 0.0105 - val_loss: 0.2507 - val_msle: 0.0390 - learning_rate: 0.0082\n",
      "Epoch 188/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0796 - msle: 0.0104 - val_loss: 0.2459 - val_msle: 0.0382 - learning_rate: 0.0081\n",
      "Epoch 189/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9s/step - loss: 0.0793 - msle: 0.0104 - val_loss: 0.2450 - val_msle: 0.0381 - learning_rate: 0.0081\n",
      "Epoch 190/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0793 - msle: 0.0104 - val_loss: 0.2460 - val_msle: 0.0383 - learning_rate: 0.0081\n",
      "Epoch 191/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0792 - msle: 0.0104 - val_loss: 0.2482 - val_msle: 0.0387 - learning_rate: 0.0081\n",
      "Epoch 192/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0784 - msle: 0.0103 - val_loss: 0.2511 - val_msle: 0.0393 - learning_rate: 0.0081\n",
      "Epoch 193/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0783 - msle: 0.0103 - val_loss: 0.2549 - val_msle: 0.0399 - learning_rate: 0.0081\n",
      "Epoch 194/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0777 - msle: 0.0101 - val_loss: 0.2594 - val_msle: 0.0408 - learning_rate: 0.0081\n",
      "Epoch 195/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0774 - msle: 0.0101 - val_loss: 0.2643 - val_msle: 0.0416 - learning_rate: 0.0081\n",
      "Epoch 196/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0773 - msle: 0.0101 - val_loss: 0.2698 - val_msle: 0.0426 - learning_rate: 0.0081\n",
      "Epoch 197/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 10s/step - loss: 0.0772 - msle: 0.0101 - val_loss: 0.2756 - val_msle: 0.0436 - learning_rate: 0.0081\n",
      "Epoch 198/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0767 - msle: 0.0100 - val_loss: 0.2817 - val_msle: 0.0447 - learning_rate: 0.0080\n",
      "Epoch 199/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0764 - msle: 0.0099 - val_loss: 0.2880 - val_msle: 0.0458 - learning_rate: 0.0080\n",
      "Epoch 200/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0760 - msle: 0.0098 - val_loss: 0.2944 - val_msle: 0.0469 - learning_rate: 0.0080\n",
      "Epoch 201/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0754 - msle: 0.0097 - val_loss: 0.3010 - val_msle: 0.0480 - learning_rate: 0.0080\n",
      "Epoch 202/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0753 - msle: 0.0097 - val_loss: 0.3076 - val_msle: 0.0491 - learning_rate: 0.0080\n",
      "Epoch 203/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0748 - msle: 0.0096 - val_loss: 0.3139 - val_msle: 0.0502 - learning_rate: 0.0080\n",
      "Epoch 204/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0740 - msle: 0.0094 - val_loss: 0.3200 - val_msle: 0.0512 - learning_rate: 0.0080\n",
      "Epoch 205/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 9s/step - loss: 0.0746 - msle: 0.0095 - val_loss: 0.3261 - val_msle: 0.0521 - learning_rate: 0.0080\n",
      "Epoch 206/1000\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 10s/step - loss: 0.0726 - msle: 0.0091 - val_loss: 0.3318 - val_msle: 0.0528 - learning_rate: 0.0080\n"
     ]
    }
   ],
   "source": [
    "def split_series(series, n_past, n_future):\n",
    "    X, y = [], []\n",
    "    for window_start in range(len(series)):\n",
    "        past_end = window_start + n_past\n",
    "        future_end = past_end + n_future\n",
    "        if future_end > len(series):\n",
    "            break\n",
    "        X.append(series[window_start:past_end, :])\n",
    "        y.append(series[past_end:future_end, :])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "n_future = 16\n",
    "n_past = scaled_validation_samples.shape[0] - n_future\n",
    "n_features = num_stores_train * num_families_train\n",
    "\n",
    "if (n_past - n_future)%2 == 0:\n",
    "    crop_max = int((n_past - n_future))\n",
    "else:\n",
    "    crop_min = int((n_past - n_future)/2)\n",
    "    crop_max = 2*crop_min + 1\n",
    "\n",
    "X_train, y_train = split_series(scaled_train_samples, n_past, n_future)\n",
    "X_val, y_val = split_series(scaled_validation_samples, n_past, n_future)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same', input_shape=(n_past, n_features)))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.MaxPool1D(strides=1, padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=1024, kernel_size=(15), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=1024, kernel_size=(15), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=1024, kernel_size=(15), padding='same'))\n",
    "model.add(layers.MaxPool1D(strides=1, padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.SeparableConv1D(filters=512, kernel_size=(7), padding='same'))\n",
    "model.add(layers.MaxPool1D(strides=1, padding='same'))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=256, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Bidirectional(layers.LSTM(units=256, return_sequences=True)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Cropping1D(cropping=(0, crop_max)))\n",
    "model.add(layers.TimeDistributed(layers.Dense(n_features)))\n",
    "\n",
    "model.compile(loss='mae', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['msle'])\n",
    "model.build((None, n_past, n_features))\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_msle', min_delta=1e-4, patience=100, restore_best_weights=True)\n",
    "\n",
    "model_history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, callbacks=[lr_cb, early_stopping], \n",
    "                          batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490023ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:05:06.218684Z",
     "iopub.status.busy": "2024-07-26T08:05:06.218326Z",
     "iopub.status.idle": "2024-07-26T08:05:06.795815Z",
     "shell.execute_reply": "2024-07-26T08:05:06.794869Z"
    },
    "papermill": {
     "duration": 0.748359,
     "end_time": "2024-07-26T08:05:06.797773",
     "exception": false,
     "start_time": "2024-07-26T08:05:06.049414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 875ms/step\n"
     ]
    }
   ],
   "source": [
    "x_test_pred = scaled_validation_samples[-n_past:].reshape((1, n_past, n_features))\n",
    "scaled_y_predict = model.predict(x_test_pred)\n",
    "\n",
    "y_predict = pd.DataFrame(minmax_scaler.inverse_transform(scaled_y_predict.reshape((n_future, n_features))), columns=valid_samples_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "effd2ca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:05:07.139459Z",
     "iopub.status.busy": "2024-07-26T08:05:07.138630Z",
     "iopub.status.idle": "2024-07-26T08:05:20.854097Z",
     "shell.execute_reply": "2024-07-26T08:05:20.852995Z"
    },
    "papermill": {
     "duration": 13.887426,
     "end_time": "2024-07-26T08:05:20.856161",
     "exception": false,
     "start_time": "2024-07-26T08:05:06.968735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of the data:\n",
      "               sales\n",
      "id                  \n",
      "3000888     3.856622\n",
      "3000889     0.000000\n",
      "3000890     2.576725\n",
      "3000891  2080.713379\n",
      "3000892     0.000000\n"
     ]
    }
   ],
   "source": [
    "pivoted_test = test_data.pivot(index=['date','dcoilwtico','dayofweek'], columns=['store_nbr', 'family'], values='id')\n",
    "\n",
    "for day_ith, day_ith_pred in y_predict.iterrows():\n",
    "    for n_samples_per_day in range(len(day_ith_pred)):\n",
    "        sample_id = pivoted_test.iloc[[day_ith], [n_samples_per_day]].values[0][0]\n",
    "        values = max(0, day_ith_pred.values[n_samples_per_day])\n",
    "        submission.at[sample_id, 'sales'] = max(0, day_ith_pred.values[n_samples_per_day])\n",
    "\n",
    "submission.to_csv('submission.csv')\n",
    "print(\"\\nFirst 5 rows of the data:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 286.363123,
   "end_time": "2024-07-26T08:05:24.383801",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-26T08:00:38.020678",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
